# -*- coding: utf-8 -*-
"""Tensorflow_Araba_Siniflandirma_projesi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gb84hhIU6bZ7wyShpBwGnYl1oef1A1tW
"""

# Sınıflandrma işlemi için veri setinde verilerin 0-1 şeklinde zararlı-zararsız anlamı taşıyan bir veriseti kullanacağız

import pandas as pd
import numpy as np





dataFrame = pd.read_excel("/content/sample_data/maliciousornot.xlsx")

dataFrame

dataFrame.info()

dataFrame.describe()

dataFrame.corr()["Type"].sort_values()  # korelasyonuna bakalım, burada dataFrame içerisindeki kolonların Type göre korelasyonunu bize gösterir

# Type'ı Hangileri negatif hangileri pozitif etkiliyor onu görebiliyoruz

import matplotlib.pyplot as plt
import seaborn as sbn

sbn.countplot(x = "Type", data = dataFrame)  # Bu bize kaç tane 0, kaç tane 1 olduğunu gösterir

dataFrame.corr()["Type"].sort_values().plot(kind = "bar") #korelasyonumuzu grafiksel olarak bu şekilde de görebiliriz

y = dataFrame["Type"].values    # values diyerek bunu bir numpy dizisine çeviriyoruz
x = dataFrame.drop("Type", axis = 1)   # kolon olara Type'ı düşürüp geri kalan her şeyi x'e atiyoruz

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 15)

# scling için

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()  # Min-Max ölçekleyiciyi oluştur

scaler.fit(x_train) # Eğitim verilerini kullanarak ölçekleyiciyi eğit

x_train = scaler.transform(x_train)   # Eğitilmiş ölçekleyiciyi kullanarak eğitim verilerini ölçekle

x_test = scaler.transform(x_test)   # Aynı ölçekleyiciyi kullanarak test verilerini ölçekle

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Dropout ve EarlyStopping'i özellikle overfitting(aşırı öğrenme) problemlerini çözmek için kullanacağız

x_train.shape

model = Sequential()

model.add(Dense(units = 30, activation = "relu"))  # Genelde kaç tane kolon varsa giriş layer(katmanı) o kadar sayıda nöron içerir.
model.add(Dense(units = 15, activation = "relu"))  # başlangış layer nöron sayısı (30) ile çıkış katmanı nöron sayısı (1) arasında bir değer verilir ara katman nöron sayısı olaral
model.add(Dense(units = 15, activation = "relu"))
model.add(Dense(units = 1, activation = "sigmoid"))  # sınıflandırma problemlerinde çıkış layerına da activasyon fonksiyonu kullanılır
                                                     # ve genelde sigmoid kullanılır 0-1 arası değer verdiği için

model.compile(loss = "binary_crossentropy", optimizer = "adam")

# Sınıflandırma işlemi yani 0-1 ,binary bi işlem yaptığımız için buradaki loss (kayıp,maliyet fonk) fonksiyonu binary_crossentropy olacak

model.fit(x = x_train, y = y_train, epochs = 700, validation_data = (x_test,y_test), verbose = 1) # epoch yüksek verelim bakalım ne olacak ?

modelKaybi = pd.DataFrame(model.history.history)

modelKaybi.plot()

"""* Normalde olması gereken ikisinin de azalarak gitmesi ama burada Val_loss belirli bir yerden sonra çok artıyor, loss çok abartı azalıyor.
* Model 0-1 sınıflandırılmasından öte yeni bir veri verince bu 0-1 işlemi yapılmasından öte elinde bulundurduğu verileri ona uydurmaya çalışıyor.
* Yani elimizdeki bir veriyi verirsek belki doğru sonuç verebilir ama yeni bir veri verdiğimizde oldukça yanlış bir sonuç verecektir.
* Kendi içinde çok fazla tutarlı olmaya çalışıyor. O yüzden epochu belirli bir oranda durdurmamız gerekiyor. Epoch yüksek verildiğinde overfitting(ezberleme-aşırı öğrenme) oluyor.
* Val_loss'u izleyip eğer val_loss saçma sapan artmaya başlarsa o zaman durdur artık train etme diyebiliyoruz. İşte bunun için **EarlyStopping** dediğimiz şeyi kullanıyoruz.
"""



model = Sequential()

model.add(Dense(units = 30, activation = "relu"))
model.add(Dense(units = 15, activation = "relu"))
model.add(Dense(units = 15, activation = "relu"))
model.add(Dense(units = 1, activation = "sigmoid"))

model.compile(loss = "binary_crossentropy", optimizer = "adam")

earlyStopping = EarlyStopping(monitor = "val_loss", patience = 25, mode = "min", verbose = 1, )
# monitor : gözlemlenecek olan şey
# patience : epochlara bakıyor, mesela 5 epoch sonrası modelimizde herhangi bir iyileşme olmazsa durduruyor. genelde 20-25 de tutulur
# mode = min : dersen takip ettiğin şeyi en az da tutmaya çalıştığın anlamına gelir. mesela val_loss(kayıp) değeri az da tutmaya çalışılır
# mode = max dersek en yukarıda tutmaya çalıştığın anlamına gelir. Mesela accuracy(doğruluk) değeri yukarıda tutulmaya çalışılır

# callbacks ekledik
model.fit(x = x_train, y = y_train, epochs = 700, validation_data = (x_test,y_test), verbose = 1, callbacks = [earlyStopping])

"""Gördüğümüz gibi model eğitimimiz 228. epoch da **early stopping** yaparak durdu. Demek ki epoch değerimizi 228 yapmak yeterliymiş. İşte biz bunu bilemeyiz o yüzden earlystopping kullanarak bunu görebiliyoruz. O yüzden earlystoppping kullanmak oldukça mantıklıdır.

"""

modelKaybi = pd.DataFrame(model.history.history)

modelKaybi.plot()

"""Gördüğümüz gibi gerçekten de baya bir iyileşme var. earlystopping ile val_loss biraz daha mantıklı bir hale geldi

* Peki hala memnun değilsek ne yapabiliriz ?

**Dropout**
verdiğimiz *rate* değeri kaç ise o kadar o katman için o kadar nöronu modelimizden rastgele bir şekilde atmaya, devredışıbırakmaya başlayacak. Böylece eğer layer'larla ilgili bir overfitting yaşıyorsak onlarda random bir şekilde training süresince nöronlar atılarak, val_loss daha kontrollü bir halde tutulacak ve overfittingi daha iyi engelleyerek daha verimli bir hale gelmiş olacak modelin verimliliği artacak

* Eğer overfitting yoksa kullanmamıza çok da gerek yok
"""

model = Sequential()

model.add(Dense(units = 30, activation = "relu"))
model.add(Dropout(0.6)) # her katman için dropout ayrı ayrı eklenir. nöronların % kaçında turn-off(kapatma) yapacağımızı belirtiyoruz.
                        # 0.5 üzerinde yapınca genelde hata verir model bol-zuluyor diye
model.add(Dense(units = 15, activation = "relu"))
model.add(Dropout(0.6))

model.add(Dense(units = 15, activation = "relu"))
model.add(Dropout(0.6))

model.add(Dense(units = 1, activation = "sigmoid"))

model.compile(loss = "binary_crossentropy", optimizer = "adam")

model.fit(x = x_train, y = y_train, epochs = 700, validation_data = (x_test,y_test), verbose = 1, callbacks = [earlyStopping])

kayipDf = pd.DataFrame(model.history.history)

kayipDf.plot()

"""* gördüğümüz gibi özellikle **dropout** kullandığımızda bu şekilde **zikzaklı** olması çok normal
* Tam istediğimiz bir sonuç yok ama daha önceye göre daha iyi görünüyor
* Böylece bir overfitting durumunun biraz biraz üstesinden gelmeyi başardık
* Daha da optimize edebiliriz
"""

tahminlerimiz = model.predict_classes(x_test)  # hatanın nedeni , predict_classes özelliğinin artık TensorFlow 2.x sürümünden sonra kullanılmamasıdır.

#tahminlerimiz = model.predict(x_test)
#tahminlerimiz = np.argmax(tahminlerimiz, axis=1)

tahminlerimiz = (model.predict(x_test) > 0.5).astype("int32")

"""Bu kod parçası, bir modelin bir veri kümesi üzerindeki tahminlerini alır ve bu tahminlerin eşik değeri (threshold) üzerinde olup olmadığını kontrol ederek sonucu ikili bir diziye dönüştürür. İşte bu kodun açıklaması:

1. `model.predict(x_test)`: Model, `x_test` veri kümesi üzerinde tahminler yapar ve her giriş için bir olasılık dizisi döndürür. Her olasılık, belirli bir sınıfın veya olayın gerçekleşme olasılığını temsil eder.

2. `(model.predict(x_test) > 0.5)`: Burada, her bir olasılık değeri, belirli bir eşik değeri olan 0.5 ile karşılaştırılır. Eğer bir olasılık değeri 0.5'ten büyükse, bu, modelin o sınıfın gerçekleştiğini tahmin ettiği anlamına gelir.

3. `.astype("int32")`: Karşılaştırma sonucu, boolean (True/False) değerler içerir. Bu adım, bu boolean değerleri tamsayı değerlerine dönüştürür. 0.5'ten büyükse `True` olarak kabul edilir ve tamsayı olarak 1'e dönüştürülür. 0.5'ten küçükse `False` olarak kabul edilir ve tamsayı olarak 0'a dönüştürülür.

Sonuç olarak, `predictions` adlı dizi, her bir girişin 0 veya 1 ile temsil edilen tahminlerini içerir. Bu sıklıkla ikili sınıflandırma (binary classification) problemlerinde kullanılır, özellikle sonuçların olasılık değerleri ile değil, belirli bir eşik değeri üzerinde mi altında mı olduğunu kontrol etmek istediğinizde.
"""

tahminlerimiz

from sklearn.metrics import classification_report, confusion_matrix

# confusion_matrix : sınıflandırmanızın ne kadar doğru sonuçlar verdiğinin değerlendirmek için oluşturulan bir matrix

classification_report(y_test, tahminlerimiz)

print(classification_report(y_test, tahminlerimiz))

"""* Burada precision(kesinlik) yani ne kadar duyarlı bir şekilde tahmin ettiğini gösteriyor
* 0 -> 0.91 kesinlik oranı ile
* 1 -> 0.93 kesinlik oranı ile tahmin ediyormuş
"""

print(confusion_matrix(y_test, tahminlerimiz))

"""* Buradaki çıktıdaki 5 değeri testlerimiz içerisindeki 5 tane değeri yanlış bulmuş geri kalanları doğru bulmuş anlamına geliyor"""

